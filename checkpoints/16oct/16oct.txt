Went deep into gatk documentation, and tried to filter out high copy number ratio intervals, as thought they are spurious. Multiple methods of filtering didn't seem to work- mappingquailty, mappingqualityavailable, readlength, etc.- these methods did not seem to work.

I saw that even after performing the filtering, the read counts file had similar size, and just the intervals with a copy number value non abiding by the filter were made zero. I wanted to simply remove those intervals, as even without any read filters, the count file had a lot of 0 count number intervals, which was ruining the whole segmentation analysis later on.

Then, I somehow landed upon a filter for intervals called filterintervals. It filtered out those intervals, which had an unusually high or low read count raw number, from the collectreadcount output. Now, the file with read counts had a much lesser size (for the tsv file, a change in file size from 64mb to less than 1mb). As expected, now only the intervals with a standard value of read counts were preserved. using this interval file for denoising and all later steps, I got pretty good segmentation results (segmentation image file added to repo).
